{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposal for the Gammapy Modeling and Fitting API\n",
    "\n",
    "This notebook outlines a proposal for restructuring and extending the current Gammapy modeling and fitting API to enable joint likelihood fitting. The proposal includes the introduction of new abstractions, represented by the following classes:\n",
    "\n",
    "* `SkyModelIRF`: a \"forward folded\" model, that applies IRFs to a `SkyModel` instance and returns an integrated quantity corresponding to predicted counts. It can only be evaluated on a fixed grid, passed on input. It is basically, what the current `ModelEvaluator` does now, but with the model parameters attached.\n",
    "\n",
    "* `BackgroundModel`: already integrated model, with fixed binning. It is initialized with a background map and introduces additional background parameters, such as `norm` or `tilt`. This model generic and not specific to 1D, 2D or 3D data.\n",
    "\n",
    "* `NPredModel`: combines a list of `SkyModelIRF` and / or `BackgroundModel` and joins the parameters lists and sums up the contributions from all model components in the list. This model generic and not specific to 1D, 2D or 3D data.\n",
    "\n",
    "* `Likelihood`: takes the binned data and model, updates the parameters and re-evaluates the model on every iteration of the fit. For different fit statistics sub-classes such as `CashLikelihood`, `WStatLikelihood` or `Chi2Likelihood` are introduced. This object is passed to the `Fit` object\n",
    "\n",
    "* `JointLikelihood`: takes a list of `Likelihood` objects, joins the parameter lists and computes the joint likelihood. This object is passed to the `Fit` object. The loop over likelihoods should be implemented using parallel processing.\n",
    "\n",
    "\n",
    "Short term this code structure solves many uses cases without the introduction of `Datasets` and also replaces the current `MapFit`, as well as `FluxPointFit` and possibly `SpectrumFit` classes. Long-term we might re-introduce \n",
    "convenience classes, specific to certain tasks.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The use cases that can be solved with the proposed structure are listed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gammapy.cube import SkyModelIRF, BackgroundModel, NPredModel\n",
    "from gammapy.utils.fitting import CashLikelihood, JointLikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gammapy.maps import WcsGeom\n",
    "\n",
    "geom_counts = WcsGeom()\n",
    "geom_exposure = WcsGeom()\n",
    "\n",
    "\n",
    "# counts and background maps are grouped in one estimator, as the background might be \n",
    "# renormalized or fitted on basis of the counts map.\n",
    "cme = CountsMapEstimator(geom_counts, offset_max=\"2.5 deg\")\n",
    "maps = cme.run(observations)\n",
    "print(maps[\"counts\"])\n",
    "print(maps[\"background\"])\n",
    "\n",
    "\n",
    "# Reduced IRFs and exposure map are grouped in one estimator as they require both geometries.\n",
    "# Reduced psf and edisp are computed such, that they map the geom of the exposure map to the \n",
    "# geom of the counts map after application, for edisp this is achieved via a matrix dot product,\n",
    "# for psf it could be a convolution with subsequent downsampling.\n",
    "ime = IRFMapEstimator(geom_exposure)\n",
    "# or \n",
    "# ime = IRFMapEstimator(geom_counts, e_true, oversampling_factor)\n",
    "\n",
    "maps = ime.run(observations)\n",
    "print(maps[\"exposure\"])\n",
    "print(maps[\"psf\"])\n",
    "print(maps[\"edisp\"])\n",
    "\n",
    "\n",
    "dnde_model = SkyModels()\n",
    "\n",
    "# this convenience method looks up the irfs for every model component, makes a cutout of the \n",
    "# exposure map and creates a SkyModelIRF instance for every component and joins them in the\n",
    "# NPredModel\n",
    "npred_model = NPredModel.from_irfs(dnde_model, irfs, background=BackgroundModel())\n",
    "\n",
    "model = SkyModelIRF(\n",
    "    exposure=exposure,\n",
    "    psf=psf,\n",
    "    edisp=edisp.\n",
    "    dnde_model\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case 1: Simple Fit with Background Norm\n",
    "\n",
    "Simple fit of a model to one observation, equivalent to what `MapFit` does now but with additional background parameters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnde_model = SkyModel()\n",
    "\n",
    "# the exposure-map can have a different geometry than the background and exposure map, only after applying\n",
    "# the PSF and Edisp it must be the same. This a natural place to introduce over oversampling for evaluating\n",
    "# differential models\n",
    "\n",
    "model = IRFSkyModel(dnde_model, exposure_map, psf, edisp)\n",
    "background = BackgroundModel(background_map)\n",
    "\n",
    "npred = NPredModel([model, background])\n",
    "like = CashLikelihood(counts_map, npred)\n",
    "\n",
    "fit = Fit(like)\n",
    "\n",
    "fit.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRFSkyModel(MapModel): # ???\n",
    "    \"\"\"Model combining IRFs and a sky model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : `SkyModel`\n",
    "        Sky model\n",
    "    exposure : `Map`\n",
    "        Exposure map to evaluate the model on.\n",
    "    psf : `PSFKernel`\n",
    "        PSF kernel\n",
    "    edisp : EDispMatrix\n",
    "        Edisp matrix.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, exposure, psf, edisp):\n",
    "        self.model = model\n",
    "        # maybe we will have hidden IRFs parameters to evaluate systematics\n",
    "        self.edisp = edisp\n",
    "        self.psf = psf\n",
    "        self.exposure = exposure\n",
    "        \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        \"\"\"Model parameters\"\"\"\n",
    "        return self.model.parameters\n",
    "    \n",
    "    def dnde(self):\n",
    "        \"\"\"Compute dnde\"\"\"\n",
    "        dnde = self.model(self.lon, self.lat, self.energy)\n",
    "        return self.exposure.copy(data=dnde)\n",
    "        \n",
    "    def flux(self):\n",
    "        \"\"\"Compute flux\"\"\"\n",
    "        flux = self.dnde * self.bin_volume\n",
    "        return flux\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"Evaluate model\"\"\"\n",
    "        npred = self.flux * self.exposure\n",
    "        if self.edisp:\n",
    "            npred.apply_edisp(self.edisp)\n",
    "        if self.psf:\n",
    "            npred.apply_psf(self.psf)\n",
    "        return npred\n",
    "        \n",
    "    def poisson(self, random_seed=42):\n",
    "        \"\"\"poisson model\"\"\"\n",
    "        npred = self.evaluate()\n",
    "        return np.random.poisson(npred)\n",
    "        \n",
    "\n",
    "        \n",
    "class BackgroundModel(MapModel):\n",
    "    \"\"\"Background model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    background : `Map`\n",
    "        Background model map\n",
    "    norm : float\n",
    "        Norm parameter.\n",
    "    tilt : float\n",
    "        Tilt parameter.\n",
    "    reference : `Quantity`\n",
    "        Reference energy of the tilt.\n",
    "    \"\"\"\n",
    "    def __init__(self, background, norm=1, tilt=0, reference=\"1 TeV\"):\n",
    "        self.map = background\n",
    "        self.parameters = Parameters([\n",
    "            Parameter(\"norm\", norm, unit=\"\"),\n",
    "            Parameter(\"tilt\", tilt, unit=\"\", fixed=True),\n",
    "            Parameter(\"reference\", reference, fixed=True),\n",
    "        ])\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"Evaluate background model\"\"\"\n",
    "        norm = self.parameters[\"norm\"].value\n",
    "        tilt = self.parameters[\"tilt\"].value\n",
    "        refernce = self.parameters[\"reference\"].value\n",
    "        tilt_factor = np.power(energy / reference, -tilt)\n",
    "        return norm * self.map.data * tilt_factor\n",
    "        \n",
    "    def poisson(self):\n",
    "        \"\"\"poisson background model\"\"\"\n",
    "        npred = self.evaluate()\n",
    "        return np.random.poisson(npred)\n",
    "\n",
    "    def read(self):\n",
    "        pass\n",
    "        \n",
    "        \n",
    "class NPredModel(MapModel):\n",
    "    \"\"\"\n",
    "    Predicted number of counts model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    components : list of `BackgroundModel` and `IRFSkyModel`\n",
    "        Components of the npred model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, components, background):\n",
    "        components_parameters = [_ for _ in components]\n",
    "        self.parameters = Parameters(components_parameters + background.parameters)\n",
    "        self.background = background\n",
    "        self.components = components\n",
    "        \n",
    "    def poisson(self, random_seed=42):\n",
    "        \"\"\"Draw counts from NPRedModel\"\"\"\n",
    "        npred = self.evaluate()\n",
    "        return np.random.poisson(npred)\n",
    "        \n",
    "    def evaluate(self):\n",
    "        # this is a good place to handle bounding boxes\n",
    "        npred = self.background.evaluate()\n",
    "        for component in self.components:\n",
    "            npred.fill_by_Coord(component.evaluate()) \n",
    "        return npred\n",
    "        \n",
    "    def read():\n",
    "        pass\n",
    "    \n",
    "    def write():\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case 2: Joint Fit across Multiple Observations\n",
    "\n",
    "This use case involves a joint fit across mutiple observations with varying IRFs, but the same model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnde_model = SkyModel()\n",
    "\n",
    "model_1 = SkyModelIRF(exposure_map_1, psf_1, edisp_1, dnde_model)\n",
    "model_2 = SkyModelIRF(exposure_map_2, psf_2, edisp_2, dnde_model)\n",
    "\n",
    "\n",
    "background_1 = BackgroundModel(background_map_1)\n",
    "npred_1 = NPredModel([model_1, background_1])\n",
    "\n",
    "background_2 = BackgroundModel(background_map_2)\n",
    "npred_2 = NPredModel([model_2, background_2])\n",
    "\n",
    "\n",
    "like_1 = CashLikelihood(counts_map_1, npred_1)\n",
    "like_2 = CashLikelihood(counts_map_2, npred_2)\n",
    "\n",
    "\n",
    "joint_like = JointLikelihood([like_1, like_2])\n",
    "\n",
    "fit = Fit(joint_like)\n",
    "fit.optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case 3: Stacking Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking obervations\n",
    "cme = CountsMapEstimator(geom_counts)\n",
    "ime = IRFMapEstimator(geom_counts, geom_exposure)\n",
    "\n",
    "# Data preparation\n",
    "for ids in observations_table.groups:\n",
    "    observations = data_store.get_observations(ids)\n",
    "    maps = cme.run(sub_observations)\n",
    "    irfs = ime.run(sub_observations)\n",
    "    \n",
    "    maps.write(\"maps.fits\")\n",
    "    irfs.write(\"reduced_irf.fits\")\n",
    "    \n",
    "\n",
    "# Model fitting\n",
    "dnde_model = SkyModels()\n",
    "\n",
    "joint_likelihood = []\n",
    "\n",
    "for maps, irfs in zip():\n",
    "    counts = maps[\"counts\"]\n",
    "    background = BackgroundModel(maps[\"background\"])\n",
    "    npred = NPredModel.from_irfs(dnde_model, irfs, background=background)\n",
    "    \n",
    "    joint_likelihood.append(CashLikelihood(counts, npred))\n",
    "    \n",
    "    \n",
    "like = JointLikelihood(joint_likelihood)\n",
    "fit = Fit(like)\n",
    "result = fit.optimize()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case 3: IRFs per Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnde_model_1 = SkyModel()\n",
    "dnde_model_2 = SkyModel()\n",
    "\n",
    "\n",
    "model_1 = SkyModelIRF(exposure_map, psf_1, edisp_1, dnde_model_1)\n",
    "model_2 = SkyModelIRF(exposure_map, psf_2, edisp_2, dnde_model_2)\n",
    "\n",
    "background = BackgroundModel(background_map)\n",
    "npred = NPredModel([model_1, model_2, background])\n",
    "\n",
    "like = CashLikelihood(counts_map, npred)\n",
    "\n",
    "fit = Fit(like)\n",
    "fit.optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case 4: Joint Likelihood with Fermi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnde_model = SkyModel()\n",
    "diffuse_model = SkyCubeDiffuse()\n",
    "\n",
    "model = SkyModelIRF(exposure_map, psf, edisp, dnde_model)\n",
    "\n",
    "model_fermi = SkyModelIRF(exposure_map_fermi, psf_fermi, edisp=None, dnde_model + diffuse_model)\n",
    "\n",
    "# or alternatively precompute the diffuse emission as background model\n",
    "fermi_diffuse = SkyModelIRF(exposure_map_fermi, psf_fermi, edisp=None, diffuse_model)\n",
    "background_fermi = BackgroundModel(fermi_diffuse.evaluate())\n",
    "\n",
    "\n",
    "background = BackgroundModel(background_map)\n",
    "npred = NPredModel([model, background])\n",
    "\n",
    "npred_fermi = NPredModel([model_fermi])\n",
    "\n",
    "# or with the precomputed background\n",
    "npred_fermi = NPredModel([model_fermi, background_fermi])\n",
    "\n",
    "\n",
    "like = CashLikelihood(counts_map, npred)\n",
    "like_fermi = CashLikelihood(counts_fermi, npred_fermi)\n",
    "\n",
    "joint_like = JointLikelihood([like, like_fermi])\n",
    "\n",
    "fit = Fit(joint_like)\n",
    "fit.optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Case 5: Joint Likelihood with 3D and Flux Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral_model = SpectralModel()\n",
    "spatial_model = SpatialModel()\n",
    "\n",
    "dnde_model = SkyModel(spectral_model, spatial_model)\n",
    "\n",
    "model = SkyModelIRF(exposure_map, psf, edisp, dnde_model)\n",
    "\n",
    "background = BackgroundModel(background_map)\n",
    "npred = NPredModel([model, background])\n",
    "\n",
    "like = CashLikelihood(counts_map, npred)\n",
    "like_fp = Chi2Likelihood(fp_data, spectral_model)\n",
    "\n",
    "joint_like = JointLikelihood([like, like_fp])\n",
    "\n",
    "fit = Fit(joint_like)\n",
    "fit.optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is **just a first proposal** and of course there are many details to be sorted out. Open questions are:\n",
    "* Where does the model.evaluate() call happen? Right now it is in the `Likelihood` class (see draft implementation below), does this make sense?\n",
    "* How to handle masks during the likelihood evaluation and masks for joint-likelihoods (see draft implementation below)?\n",
    "* Is the extra-abstraction of an `NPredModel` needed? \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Likelihood(object):\n",
    "    \"\"\"Binned likelihood object\"\"\"\n",
    "    def __init__(data, model, mask=None):\n",
    "        self.data = data\n",
    "        self.model = model\n",
    "        self.mask = mask\n",
    "    \n",
    "    # this method is the interface to the optimizer \n",
    "    # could also be named evaluate_fit() or similar\n",
    "    def __call__(self, factors, mask=None):\n",
    "        \"\"\"Evaluate the likelihood for the fit\"\"\"\n",
    "        self.model.parameters.set_factors(factors)      \n",
    "        return self.total(mask)\n",
    "    \n",
    "    def per_bin(self):\n",
    "        \"\"\"Evaluate the likelihood per bin\"\"\"\n",
    "        # IMPORTANT: it is not clear where the best place for the model evaluation call is.\n",
    "        model = self.model.evaluate()\n",
    "        likelihood = self.evaluate(self.data, model)\n",
    "        return likelihood\n",
    "    \n",
    "    def total(self, mask=None):\n",
    "        \"\"\"Evaluate the total summed likelihood and apply optionally a mask\"\"\"        \n",
    "        if self.mask:\n",
    "            mask_default = self.mask.data\n",
    "            if mask:\n",
    "                # TODO: re-think how to combine the masks\n",
    "                mask = mask & mask_default\n",
    "            stat = self.per_bin[mask]\n",
    "        else:\n",
    "            stat = self.per_bin\n",
    "        return np.sum(stat, dtype=np.float64)      \n",
    "\n",
    "    \n",
    "    \n",
    "class CashLikelihood(Likelihood):\n",
    "    \"\"\"Cash statistics likelihood\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    counts : `Map`\n",
    "        Counts map\n",
    "    npred : `NPredModel`\n",
    "        Predicted number of counts.\n",
    "    mask : `Map`\n",
    "        Mask to apply.\n",
    "    \"\"\"\n",
    "    def __init__(self, counts=None, npred=None, mask=None):\n",
    "        self.counts = counts\n",
    "        self.npred = npred\n",
    "        self.mask = mask   \n",
    "        \n",
    "    @property\n",
    "    def model(self):\n",
    "        \"\"\"Alias for a nicer documentation\"\"\"\n",
    "        return self.npred\n",
    "    \n",
    "    @property\n",
    "    def data(self):\n",
    "        \"\"\"Alias for a nicer documentation\"\"\"\n",
    "        return self.counts.data\n",
    "    \n",
    "    # This is a static method using pure numpy, because users might want\n",
    "    # to call it directly CashLikelihood.evaluate(counts, npred), instead\n",
    "    # of our current cash(counts, npred)\n",
    "    @staticmethod\n",
    "    def evaluate(counts, npred):\n",
    "        \"\"\"Cash likelihood formula\"\"\"\n",
    "        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "            value = 2 * (npred - counts * np.log(npred))\n",
    "            value = np.where(npred > 0, value, 0)\n",
    "        return value\n",
    "    \n",
    "    \n",
    "class Chi2Likelihood(Likelihood):        \n",
    "    def per_bin(self):\n",
    "        \"\"\"Evaluate the likelihood per bin\"\"\"\n",
    "        model = self.model.evaluate(parameters)\n",
    "        \n",
    "        likelihood = self.evaluate(self.data, model, model_err)\n",
    "        return likelihood\n",
    "\n",
    "    \n",
    "    @statictmethod\n",
    "    def evaluate(data, model, model_err):\n",
    "        return ((data - model) / model_err) ** 2\n",
    "\n",
    "    \n",
    "class JointLikelihood(Likelihood):\n",
    "    \"\"\"Joint likelihood across mutiple likelihoods.\"\"\"\n",
    "    def __init__(self, likelihoods, n_jobs=4):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        likelihoods : list of `Likelihood` objects\n",
    "            List of likelihoods to be combined.\n",
    "        n_jobs : int\n",
    "            Number of parallel jobs to use for the computation.\n",
    "        \"\"\"\n",
    "        self.likelihoods = likelihoods\n",
    "    \n",
    "    @property\n",
    "    def model(self):\n",
    "        \"\"\"\"\"\"\n",
    "        return ModelProxy()\n",
    "        \n",
    "    def per_bin(self):\n",
    "        # As the binning can differ for different likelihoods this\n",
    "        # does not make sense. Which in turn means you can't fit joint\n",
    "        # likelihoods with optimizers, that rely on the per bin likelihood\n",
    "        raise TypeError()\n",
    "    \n",
    "    def total(self, mask=None):\n",
    "        \"\"\"Total joint likelihood.\"\"\"\n",
    "        joint_likelihood = 0\n",
    "        \n",
    "        for like in self.likelihoods:\n",
    "            joint_likelihood += like.total(mask)\n",
    "            \n",
    "        return joint_likelihood\n",
    "       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paramters linking could be obtained via item assignment to the Parameters object\n",
    "\n",
    "from gammapy.spectrum.models import PowerLaw\n",
    "\n",
    "pwl_1 = PowerLaw()\n",
    "pwl_2 = PowerLaw()\n",
    "\n",
    "pwl_2.parameters[\"index\"].link(pwl_1.parameters[\"index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
